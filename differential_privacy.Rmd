---
title: "Differential Privacy"
author: "Matthew Drury"
output:
  ioslides_presentation:
      self_contained: yes
---

```{r, echo=FALSE}
library(ggplot2)
```

## Differental Privacy

Differential Privacy is a statistical technique providing mathematical guarantees on the preservation of individual privacy in database queries. 

Recently, many connections to statistical learning have emerged.


## Motivation for Privacy

Suppose we have a database $D$ that contains individual level income data for the citizens of the imaginary country of Eagleland.

| Person    | Residence | Income |
| --------- |:---------:|:------:|
| Ness      | Onett     | $1600  |
| Pokey     | Onett     | $500   | 
| Paula     | Twoson    | $1000  |
| Apple Kid | Twoson    | $2500  |
| Jeff      | Winters   | $1000  |
| .         | .         | .      |


## Motivation for Privacy

A researcher does not have access to the database, instead they have at their disposal a query function

$$ f(\text{Residence}) = \text{Total Income for All Residents} $$


## Motivation for Privacy

This means that the researcher has access to only aggregate views on the database.

So it should be impossible for them to determine anyone's individual income... right?


## A Simple Attack

Suppose that the researcher learns, from external sources, that Ness is moving from Onett to Twoson at some time $t_0$.

At time $t_0 - \epsilon$ they submit the query:

$$ f(\text{Onett}) $$

And at time $t_0 + \epsilon$ they submit the same query:

$$ f(\text{Onett}) $$


## A Simple Attack

The two results obtained by the researcher differ, by *exactly* Ness's income!

A small bit of external information has allowed a simple attack to obtain individual level information.


## This is Serious!

Attacking databases using external information is a real issue.

In 2008 Arvind Narayanan and Vitaly Shmatikov successfully deanonymized the Netflix competition data by attacking with the IMDB database as an external source.

Harvard researcher Latanya Sweeney attacked the  Massachusetts Group Insurance Commission medical database by linking it with voter registration records, allowing her to identify the medical history of the Governor.

## The Fundamental Issue

The result of our query $f$ is very sensitive to individual records in the database.

Differential privacy attempts to alleviate this.


## Differential Privacy: Intuition

Replace a query with a *deterministic* answer with one with a *random* answer. 

Do so in a way that the alteration of one row of data can only change the expected result of the query by a small amount $e^\epsilon$.


## Differential Privacy: Definition

A (randomized) query function $f$ gives $\epsilon$-*differential privacy* if, for all databases $D_1$ and $D_2$ differing in at most one element, and all (measurable) subsets $S$ of the range of $f$

$$ P[f(D_1) \in S] \leq e^\epsilon \times P[f(D_2) \in S] $$


## Achieving Differential Privacy

The simplest way to achieve differential privacy is called the Laplace mechanism.


## Achieving Differential Privacy

Recall the *Laplase* or *Double Exponential* distribution is a probability distribution on $\mathbf{R}$ with the density function

$$ f(x; \mu; \theta) = \frac{1}{2\theta} \exp \left( - \frac{\left| x - \mu \right| }{\theta} \right) $$


## Achieving Differential Privacy

```{r ggplot}
laplace.pdf <- function(x) 0.5 * exp(-abs(x))
x <- seq(-2, 2, length.out=100)
y <- laplace.pdf(x)
ggplot() + geom_line(aes(x=x, y=y))
```


## Achieving Differential Privacy

If $f$ is a non-random query, then the random query

$$ f_{\text{rand}}(x) = f(x) + Laplace(0, \theta) $$

is $\epsilon$-differential private for the correct choice of the dispersion parameter $\theta$.


## Ok, But What Does This Have To Do With Learning

Recall, that the *expected optimism* of a learning algorithm is

$$ \omega = E_{\text{pred}} - E_{\text{train}} $$

where the expectations average over all possible training and evaluation data sets for the learning algorithm.


## The Optimism Theorem

Efron's optimize theorem relates the optimism to the influence of an observation on its predicted value

$$ \omega = \frac{2}{n} \sum_{i = 1}^{n} cov(\hat\mu_i, y_i) $$


## The Optimism Theorem

A refinement using Stein's lemma makes this even more apparent (under certain assumptions)

$$ \omega \sim E \left( \sum_{i = 1}^{n} \frac{\partial \hat \mu}{\partial y_i} \right) $$


## The Connection, Philisophically

Given that both privacy and model optimism are related to how sensitive a query is to an observation, we should expect some connections!


## The Connection, Philisophically

If we allow a modeling algorithm to access data in a differentialally private manner, then we should expect good generalization properties.


## Example: A Noisy Catagorical

Suppose our training data contains a categorical variable $x$ with a very large number of levels, which we expect to be related to a binary response.


## Example: A Noisy Catagorical

Using such a predictor naively can cause cause a model to over fit easily.

```{r, eval = FALSE}
# Not such a good idea, estimates too many parameters!  Overfit!
M <- lm(y ~ factor(x))
```


## Example: A Noisy Catagorical

We may hit upon the idea of summarizing the response over the categorical

$$ sx_i = \frac{1}{\# \{ x = x_i \} }  \sum_{j \mid x_j = x_i} y_j $$

but this just pushes the over fitting to the summarization instead of the model fitting.


## Example: A Noisy Catagorical

We can resolve this by viewing the ingredients in the summarization as a query into our training set

$$ f(s) = \sum_{j \mid x_j = s} 1 $$
$$ g(s) = \sum_{j \mid x_j = s} y_j $$


## Example: A Noisy Catagorical

Using the Laplace mechanism renders these queries differentiably private

$$ f(s) = \sum_{j \mid x_j = s} 1 + Laplace(0, \theta)$$
$$ g(s) = \sum_{j \mid x_j = s} y_j + Laplace(0, \theta)$$

## Example: A Noisy Catagorical

Efron's theorem now informs us that this procedure should have good generalization properties!


## Example: A Re-usable holdout

Often a scientist will use a *hold out* data set during the construction of a model to 

  - Asses the expected performance of a statistical model
  - Assist in making meta-decisions about the model: variable selection, tuning parameters, etc.


## Example: A Re-usable holdout

Unfortunately, these uses are in direct conflict.

The more decisions that are made with the assistance of a hold out, the more biased its estimate of the model's expected performance becomes.


## Example: A Re-usable holdout

Dwork, Feldman, Hardt, Pitassi, Reingold, and Roth describe a differntially private way to query a hold out that prevents over fitting to the result.


## Example: A Re-usable holdout

Suppose we have a learner $\phi$ trained on a data set $S_{t}$, and a hold out $S_{h}$.

The algorithm requires a parameter $T$, the threshold.

The modeler requests the performance of the learner $\phi$.


## Example: A Re-usable holdout

Sample $\nu \sim Laplace(0, \theta_0)$.

If 

$$\| E_{t}(\phi) - E_{h}(\phi) \| > T + \nu$$ 

then output 

$$E_{h} + Laplace(0, \theta_1)$$

Otherwise, output $E_{t}(\phi)$.


## Example: A Re-usable holdout

According to Dwork, et. al. this mechanism allows the modeler to avoid over fitting to their hold out in a differentiably private manner for a surprisingly long time.

Moritz Hardt and Arvim Blum describe a modification of the algorithm above that applies to Machine Learning competition leader boards.


## Summary

Differential Privacy, while originating from a standpoint of data security, has shown very promising connections to Statistical Learning.

This discipline is still in it's infancy, so there is certainly more to come!  Keep following!

## References

Dwork, C.: Differential Privacy: A Survey of Results (2008)

Dwork C., Feldman, V., Hardt, M. Pitassi, T. Reingold, O., Roth, A.: Generalization in Adaptive Data Analysis and Holdout Reuse (2015)

Narayanan A., Shmatikov, V.: Robust De-anonymization of Large Sparse Datasets (2008)

Hardt M., Blum A.: The Ladder: A Reliable Leaderboard for Machine Learning Competitions (2015)


## References

Kaufman S., Rosset S.: When Does More Regularization Imply Fewer Degrees of Freedom?  Sufficient Conditions and Counter Examples from Lasso and Ridge Regression (2013)

Hastie T., Tibshirani R., Friedman J.: The Elements of Statistical Learning (2009)

Wikipeida: Differential Privacy